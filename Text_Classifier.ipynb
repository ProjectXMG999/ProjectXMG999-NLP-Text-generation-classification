{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just a training code/ Theoretical knowledge of Markov Models\n",
    "- Training separate models for each class\n",
    "- using Bayes Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_files = [\n",
    "  'edgar_allan_poe.txt',\n",
    "  'robert_frost.txt',\n",
    "]\n",
    "\n",
    "!head edgar_allan_poe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into lists\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for label, f in enumerate(input_files):\n",
    "  print(f\"{f} corresponds to label {label}\")\n",
    "\n",
    "  for line in open(f):\n",
    "    line = line.rstrip().lower()\n",
    "    if line:\n",
    "      # remove punctuation\n",
    "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "      input_texts.append(line)\n",
    "      labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Ytrain), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "word2idx = {'<unk>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate word2idx\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "      if token not in word2idx:\n",
    "        word2idx[token] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into integer format\n",
    "train_text_int = []\n",
    "test_text_int = []\n",
    "\n",
    "for text in train_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx[token] for token in tokens]\n",
    "  train_text_int.append(line_as_int)\n",
    "\n",
    "for text in test_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx.get(token, 0) for token in tokens]\n",
    "  test_text_int.append(line_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_int[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize A and pi matrices - for both classes\n",
    "V = len(word2idx)\n",
    "\n",
    "A0 = np.ones((V, V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V, V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute counts for A and pi\n",
    "def compute_counts(text_as_int, A, pi):\n",
    "  for tokens in text_as_int:\n",
    "    last_idx = None\n",
    "    for idx in tokens:\n",
    "      if last_idx is None:\n",
    "        # it's the first word in a sentence\n",
    "        pi[idx] += 1\n",
    "      else:\n",
    "        # the last word exists, so count a transition\n",
    "        A[last_idx, idx] += 1\n",
    "\n",
    "      # update last idx\n",
    "      last_idx = idx\n",
    "\n",
    "\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 0], A0, pi0)\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 1], A1, pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize A and pi so they are valid probability matrices\n",
    "# convince yourself that this is equivalent to the formulas shown before\n",
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log A and pi since we don't need the actual probs\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute priors\n",
    "count0 = sum(y == 0 for y in Ytrain)\n",
    "count1 = sum(y == 1 for y in Ytrain)\n",
    "total = len(Ytrain)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "p0, p1\n",
    "\n",
    "# P0 is about 33 percent and P one is about sixty six percent because of this, it wouldn't make sense to use the maximum likelihood method.\n",
    "# Instead, we should look at the posterior, which corresponds to the map method.\n",
    "# Note also that because the classes are slightly imbalanced, we may want to use a metric other than\n",
    "# the accuracy to evaluate a classifier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "class Classifier:\n",
    "  def __init__(self, logAs, logpis, logpriors):\n",
    "    self.logAs = logAs\n",
    "    self.logpis = logpis\n",
    "    self.logpriors = logpriors\n",
    "    self.K = len(logpriors) # number of classes\n",
    "\n",
    "  def _compute_log_likelihood(self, input_, class_):\n",
    "    logA = self.logAs[class_]\n",
    "    logpi = self.logpis[class_]\n",
    "\n",
    "    last_idx = None\n",
    "    logprob = 0\n",
    "    for idx in input_:\n",
    "      if last_idx is None:\n",
    "        # it's the first token\n",
    "        logprob += logpi[idx]\n",
    "      else:\n",
    "        logprob += logA[last_idx, idx]\n",
    "      \n",
    "      # update last_idx\n",
    "      last_idx = idx\n",
    "    \n",
    "    return logprob\n",
    "  \n",
    "  def predict(self, inputs):\n",
    "    predictions = np.zeros(len(inputs))\n",
    "    for i, input_ in enumerate(inputs):\n",
    "      posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] \\\n",
    "             for c in range(self.K)]\n",
    "      pred = np.argmax(posteriors)\n",
    "      predictions[i] = pred\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1], [logpi0, logpi1], [logp0, logp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrain = clf.predict(train_text_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain == Ytrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptest = clf.predict(test_text_int)\n",
    "print(f\"Test acc: {np.mean(Ptest == Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# read about F-score: https://en.wikipedia.org/wiki/F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Ytrain, Ptrain)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(Ytest, Ptest)\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytrain, Ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytest, Ptest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
