{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3\n",
    "\n",
    "- classification\n",
    "- linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "\n",
      "\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
      "\n",
      "\n",
      "['DESCR', 'data', 'data_module', 'feature_names', 'filename', 'frame', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "type(data) #sklearn.utilis.Bunch\n",
    "\n",
    "print(data.__dict__)\n",
    "print('\\n')\n",
    "print(dir([data]))\n",
    "print('\\n')\n",
    "print(dir(data))\n",
    "\n",
    "# or just take object press dot and tab and then scroll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names\n",
    "\n",
    "data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# split the data into train and test sets\n",
    "# this tets us simulate how our model will perform in the future\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574468085106383"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model's performance\n",
    "\n",
    "model.score(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1\n",
      " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1\n",
      " 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0\n",
      " 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1\n",
      " 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9574468085106383"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to make predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "N = len(y_test)\n",
    "\n",
    "np.sum(predictions == y_test) / N\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usin deeplearing to solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train)\n",
    "X_test2 = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train2, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression vs Classification\n",
    "\n",
    "- Classification : predict a category\n",
    "- Regression : predict a number on the real line ( e.g. i, -2,5, 3.14159, ...)\n",
    "- you may object !\n",
    "    - in code, spam = 1, not spam = 0 - arent these numbers?\n",
    "    - these numbers are just symbols to represent the category\n",
    "    - we can switch them with no effect\n",
    "- in regression, the numbers do have meaning\n",
    "\n",
    "EXAMPLE \n",
    "\n",
    "predicting house prices\n",
    "- real estate application\n",
    "- can have >1 input:\n",
    "    - average family income\n",
    "    - crime rate\n",
    "    - siez\n",
    "    - numbers of bedrooms\n",
    "    - when last renovated\n",
    "\n",
    "\n",
    "#### Many-dimensional input\n",
    "\n",
    "- qith so many inputs, you cant visualize it\n",
    "- 1 input --> line\n",
    "- 2 input --> plane\n",
    "- 2 inputs --> hyperplane (not visualizable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obligatory Businnes Example\n",
    "- the stock market\n",
    "- rule: Buy low, sell high\n",
    "- normally, people violate this rule becouse of emotions such as fear\n",
    "- from an ML(regression) perspective, what do we want to do?\n",
    "    - INPUTS:\n",
    "        - price for last 10 days \n",
    "        - was it mentioned in the news yesterday?\n",
    "        - was it postitive?\n",
    "    - PREDICT:\n",
    "        - Tomorrow's stock price\n",
    "\n",
    "\n",
    "IN code:\n",
    "- X(2-d array of shape NxD)\n",
    "    - N = number of samples \n",
    "    - D = number od features\n",
    "- Y(1-d array of length N)\n",
    "\n",
    "\n",
    "CODE = exactly the same  as before for classification\n",
    "\n",
    "\n",
    "    model - LinearRegression()\n",
    "    model.fit(X,Y)  # learnin\n",
    "    predictions = model.predict(X) # make predictions\n",
    "    score = model.score(X,Y) # evaluate\n",
    "\n",
    "\n",
    "#### What is the score\n",
    "\n",
    "- accuracy no longer makes sense ( #correct / #total )\n",
    "- doesnt make sense for regression\n",
    "- regression uses MSE - **MeanSquareError**, but this is not waht is returned by score ( taking the square difference of every prediction and its corresponding target, adn then take the average of all the square differences)\n",
    "\n",
    "\n",
    "#### Why not MSE?\n",
    "\n",
    "- house prices will range from hundreds of thousands to milions\n",
    "- grades will range from 0...100\n",
    "- an error of 100^2 is not bad for house prices, but it is bad for grades!\n",
    "- the relative scale matters \n",
    "\n",
    "#### The R^2\n",
    "- is literally the correlation coefficient squared\n",
    "\n",
    "    R^2 = 1 - SSE/SST\n",
    "\n",
    "SSE = some squared errors = this is just like mean squared error but we take the sum instead of the means so we dont divide by N\n",
    "\n",
    "\n",
    "- So for each prediction we take the difference between it and its coresponding targer square the difference and add all those square differences together.\n",
    "\n",
    "SST = Sum of squares total = This is the sum of square differences between each target and the sample mean of the targets.\n",
    "\n",
    "\n",
    "- If we did divide by N it would be:\n",
    "    1 - MSE/variance    = sample variance jedna\n",
    "\n",
    "\n",
    "- The perfect model: SSE is 0, R^2 = 1\n",
    "- dumbest model: predict the average every time, so SSE = SST --> R^2 = 0\n",
    "- R^2 < 0 if the predictions are so wildly incorrect that SSE > SST\n",
    "\n",
    "- 1 is the best\n",
    "- 0 is the dumbest model(pick the average target, regardless of input)\n",
    "\n",
    "######## we will come back to R^2 later\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSION IN CODE\n",
    "\n",
    "# Npte: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range, input\n",
    "\n",
    "# just in case we need it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#load the data\n",
    "\n",
    "df = pd.read_csv('../', sep = '\\t', header=None)\n",
    "\n",
    "# https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise\n",
    "\n",
    "# check the data \n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "# get the inputs\n",
    "\n",
    "data = df[[0,1,2,3,4]].values\n",
    "\n",
    "# get the outputs\n",
    "\n",
    "target = df[5].values\n",
    "\n",
    "# tiny update: pandas is moving from .as_matrix to the equivalent .values\n",
    "\n",
    "# normally we would put all of our imports at the top\n",
    "# but this lets us tell a story\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and tesy sets \n",
    "# this lets us simulate how or model will perform in the future\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33)\n",
    "\n",
    "# instantiate a classifer and train it\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model s performance\n",
    "model.score(X_train,y_train)\n",
    "model.score(X_test, y_test)\n",
    "\n",
    " # train score is 0.4990...\n",
    " # test score is 0.5467...\n",
    " # these dont represent accuracy since we are not doing classification\n",
    " # these numbers are the squares. So closer to one is better but 0.5 is not that bad\n",
    "\n",
    "# how you can make predictions\n",
    "predictions = model.predict(X_test)\n",
    " \n",
    "# what did we get?\n",
    "predictions\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even use random forest to solve the same problem\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# remember this is regression not classification\n",
    "\n",
    "model2 = RandomForestRegressor()\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model s performance\n",
    "model2.score(X_train, y_train)\n",
    "model2.score(X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature vector\n",
    "\n",
    "Vector:\n",
    "- just a list of numbers \n",
    "- we can visualize vectors by drawing them on the cartesian plane\n",
    "\n",
    "example:\n",
    "- teh vextor is (10,3,0)represnts student #1\n",
    "- in general, our data X is an N x D matrix\n",
    "    - N = NUMBER IF SAMPLES, D = NUMBER OF FEATURES\n",
    "    - each row is a feayure vector - row n is a feature vector for the nth sample\n",
    "- each column is a feature(e.g # hours studied)\n",
    "\n",
    "How can i come up with good features?\n",
    "- hours studied vs your heght is an obvious example\n",
    "- first optian - use your domain knowledge\n",
    "    E.g. an expert in anatomy can propably come up with more useful features for a biological dataset than an accountant\n",
    "\n",
    "a purely mahtematical approach\n",
    "- requires no domain knowledge at all\n",
    "- ex. if your original measurement is x, then x^2 is a feature, x^3 is a feature ...\n",
    "- this makes sense if you know about **Taylor expansions**\n",
    "\n",
    "\n",
    "- suppose y = ax + b is not a good fit(because its olny a line)\n",
    "- then try a>4x^4 + a>3x^3 + a>2x^2 + a>1x^1 + b\n",
    "\n",
    "- is this more powerful?\n",
    "- studying biology or finace takes a realy long time, never mind trying to become an expert in both\n",
    "- But Hand-made derived from experts can be much more useful than these 'automatic' features \n",
    "\n",
    "- we dont know anything about breast cancer ao airfoils\n",
    "- deep neural networks do 'automatic' feature extraction when learning\n",
    "\n",
    "\n",
    "We can combine these 2 methods:\n",
    "\n",
    "                                (deepneuralnetwork)\n",
    "\n",
    "raw data --> Expert-made -->    automatic feature --> classification  --> prediction\n",
    "             features           extraction\n",
    "\n",
    "In ML we often just pass the images or other data directly into neural network\n",
    "so whether we take the hybrid approach or not, it dependent both on what kind of dataset that you have and what kind of model do you use in"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## its all about geometry\n",
    "\n",
    "#### Regression\n",
    "- it s just a line of best fit\n",
    "\n",
    "- 2 ways to make it harder:\n",
    "    - #1 make it curvier than just a line\n",
    "    - #2 make it multi-dimensional\n",
    "\n",
    "#### Classification & Regression\n",
    "- inastances of supervised learning\n",
    "- training data is inputs X and targets Y\n",
    "- you can correct your guess becouse you are told the correct answer\n",
    "\n",
    "Getting targets is not easy:\n",
    "- consider Imagenet(10= milions images)\n",
    "- 1000 different classes\n",
    "- cost a lot\n",
    "\n",
    "this is where **unsupervised learning** comes in:\n",
    "\n",
    "- u have X but no Y\n",
    "- Example: clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All machine learnig interfaces are the same: a black box with 2 tasks:\n",
    "- learn and make predictions\n",
    "\n",
    "#### Caveats \n",
    "- w will kearn about exceptions to this 'rule' in the future\n",
    "- we have already encountered score()\n",
    "- for classification, it returns accuracy but for regression it returns R^2\n",
    "- unsupervised algorithms dont use labels( there is no 'Y')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models:\n",
    "- examples: linear regression, logistic regression\n",
    "- very easy to interpret\n",
    "\n",
    "### basic nonlinear models \n",
    "- naive bayes, decision tree, k-nearest neighbor\n",
    "\n",
    "### Ensemble models\n",
    "- random forest, adaboost, extraatrees, gradient boosted trees\n",
    "- XGBoost - win significant number of kaggle contests\n",
    "\n",
    "They take a very large number of individual decision trees say 200 of them but they train these trees over different subsets of the input data packs. So what you can get out of it is 200 different trees all trained in a different way. Well it turns out that when you average the predictions from these trees the result is very powerful.\n",
    "\n",
    "### support vector machine (SVM)\n",
    "\n",
    "- was the 'go-to' method for a long time\n",
    "-  today that is deep learing but svm used to beat neural network\n",
    "- powerful and nonlinear, but they do not scale\n",
    "- most dataset these days are to large\n",
    "\n",
    "### deeplearnig\n",
    "- state of the art in Cv and NLP\n",
    "- not plug-and-play(unlike random forest )\n",
    "- u wouldnt normally use sklearn:\n",
    "    - instead: theano, tensorflow, keras\n",
    "\n",
    "\n",
    "https://www.udemy.com/course/data-science-natural-language-processing-in-python/learn/lecture/11993778#content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reinforcement learning:\n",
    "- AlphaGo: beat a world champion in Go\n",
    "- AlphaZero: beats everyone in Go, learned to play only by playing against itself\n",
    "- video games\n",
    "- thay simulate phisics\n",
    "- can we learn real world physics?\n",
    "    - if yes can we build a robot to navigate the physical world\n",
    "\n",
    "\n",
    "### practical Concepts\n",
    "- train and rest splits\n",
    "- generalization\n",
    "- biad-variance\n",
    "- Cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markow models are everywhere\n",
    "- finance: the basis for the famous Black-Scholes formula\n",
    "- reinforcement learnig: Markov Decision Process(MDP)\n",
    "- Hidden Markov Model( speech recognition, computational biology)\n",
    "- Markov Chain Monte Carlo (MCMC): numerical approximation\n",
    "\n",
    "Section Outline:\n",
    "- markov property( requires knowledge of probability)\n",
    "- markov model, state transition matrix\n",
    "- 'training' a markov model(i.e 'learnig')\n",
    "\n",
    "applications:\n",
    "- building a text classifier using Bayes rule + Generating poetry\n",
    "- there are 2 fundamentally differenr ways to apply ML\n",
    "- Supervised: predict a target from a dataset of inputs + labels\n",
    "- unsupervised: learn structure of data, create new examples having the same structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the markov Property:\n",
    "- what is a natural thing to do when we want to 'model' a sequence\n",
    "\n",
    "Sequence : {x1,x2,x3,x4...,xT}\n",
    "\n",
    "Probability of seqence : p(x1,x2,...,xT)\n",
    " \n",
    " - The markov property is a very restrictive assumption on the dependency structure of the joint distribution( arrows denote dependency)\n",
    "\n",
    " X t-2 --> X t-1 --> X t --> X t+1 --> X t+2\n",
    "\n",
    " x t is independent of x t-2, x t-3, ...\n",
    "\n",
    "What if the markov property were not True?\n",
    "\n",
    "- 3-variable example ( Bayes'rule applied twice)\n",
    "\n",
    "p ( x>1, x>2, x>3 ) = p( x>1 )p( x>2, x>3 | x>1 )\n",
    "p ( x>1, x>2, x>3) = p(x>1)p( x>2 | x>1)p( x>3 | x>2 )\n",
    "\n",
    "since p( x>2, x>3 ) = p( x>2 )p( x>3 | x>2 )\n",
    "\n",
    "\n",
    "\n",
    "### Chain Rule of Probability\n",
    "- T-variable expansion (general case)\n",
    "- in general, each term depends on all preceding terms\n",
    "\n",
    "p (x>1, ... , x>T) = p(x>1)p(x>2 | x>1 )p(x>3 | x>1, x>2)...p(x>T | X>T-1, ..., x>1 )\n",
    "\n",
    "The markov ' Assumption ' \n",
    "- we assume the markov property holds even when it does not"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this section we will consider sequences of categorical symbols, which we will refer to as ' states '\n",
    "- example weather: {sunny, rainy , cloudy}\n",
    "- example parts-of-speech tags: {noun, verb, adjective, ...}\n",
    "\n",
    "we will use \"s\" for \"state\" ( other sources might use z or x)\n",
    "\n",
    "\n",
    "    s(t) = s>t = state at time t\n",
    "\n",
    "- time will also be discrete(t = 1,2, ...)(e.g. no such thing as t = 1.5)\n",
    "- we will number the states from 1,2,...,M\n",
    "- M = total number of possible states (e.g M = 3 for sunny,rainy,cloudy)\n",
    "\n",
    "we use i or j to index the state space\n",
    "\n",
    "\n",
    "    p(s>t = i) means: probability that state at time t is 'i'\n",
    "\n",
    "\n",
    "\n",
    "**Statae Distribution**\n",
    "\n",
    "\n",
    "- we have p( s>t = 1 ), p( s>t = 2 ), ..., p( s>t = M )\n",
    "- this is M probability values - together they form a **distribution**\n",
    "\n",
    "The state distribution would answer the question :\n",
    "\" What is the probability that it will be rainy on Sunday?\"\n",
    "Ans: p( s>sunday = rainy)\n",
    "\n",
    "    p( s>t ) = state distribution (length M vector)\n",
    "\n",
    "\n",
    "**State Transitions**\n",
    "\n",
    "Markov models are all about modeling sequences where each state depends only on the previous state we came from\n",
    "\n",
    "    p( s>t = j  |  s>t-1 = i )\n",
    "\n",
    "- \" Probability that state at time t is j, given that the state at the time t-1 was i \"\n",
    "- how many of these conditional probability values exist? ( for all i and j )\n",
    "- since both i and j can take any value from 1.. M, there are M^2 values\n",
    "\n",
    "**State Transition Matrix**\n",
    "\n",
    "∀ = \" dla kazdego x\n",
    "\n",
    "\n",
    "    A> ij = p( s>t = j | s> t-1 = i, ∀i = 1...M, j = 1...M)\n",
    "\n",
    "- Convention: first index = previous state, second index = next state\n",
    "- A is an MxM matrix\n",
    "- in general, we could have A(t)\n",
    "- when A doesnt depend on t: time-homogeneous Markov process\n",
    "\n",
    "\n",
    "**Initial State**\n",
    "\n",
    "    The quick brown fox jumps over the lazy dog\n",
    "\n",
    "- what is the probability of the first word?\n",
    "- to quantify the probability of the first state in a sequence, we use the **initial state distribution**\n",
    "\n",
    "typically we use the letter Pi to represent this distribution\n",
    " \n",
    "    Pi> i = p( s> 1 = i) ( for i = 1...M )\n",
    "\n",
    "Pi is a vector of size M\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "\n",
    "    A> ij = p(s> t = j | s> t-1 = i)\n",
    "    pi> i = p(s>1 = i)\n",
    "\n",
    "We've learned that a mark of model is described by two distributions the state transition matrix A and the initial state distribution pie.\n",
    "\n",
    "Given these two objects, we can ask some interesting questions.\n",
    "\n",
    "- Given A and Pi and a sequence {s>1, s> 2, ..., s> t} what is the probability of seeing that sequence?\n",
    "- how do we find A and Pi given a dataset>\n",
    "\n",
    "because this is machine learning will be given a data set and using this data set will determine some procedure for estimating the best values of A and Pi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a Markov model**\n",
    "\n",
    "- we are not ready to rigorously derive this yet, but the ituition is simple\n",
    "- Suppose we flip a coin a bunch of times - how do we estimate p(heads)\n",
    "\n",
    "p(heads) = count(H) / total tosses\n",
    "\n",
    "- that sjust a binary case. What if  we have M > 2?\n",
    "\n",
    "p('cat') = count('cat') / total word count\n",
    "\n",
    " ^\n",
    " |\n",
    "\n",
    "\n",
    "this answers the question: \" how frequent is the word 'cat' in the english language?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### estimating A and Pi (\"Training\")\n",
    "\n",
    "{estimate}Pi>i = count( s>1 = i ) / N\n",
    "\n",
    "N = number of sequences in datset\n",
    "\n",
    "Pie Of i is equal to the number of times each sequence started, with State i divided by the total number of sequences in our dataset.\n",
    "\n",
    "\n",
    "{estimate}A>ij = count( i -> j ) / count( i )\n",
    "\n",
    "Our best guess for A>ij is the number of times we transition from state i to state j, divided by the total number of times we were in state AI.\n",
    "\n",
    "eample: count(\"the cat) / count(\"the\")\n",
    "\n",
    "\n",
    "#### summary \n",
    "\n",
    "- what we want to do: model a sequence of states\n",
    "- State transition matrix(A), initial state distribution(Pi)\n",
    "- Task #1: find the probability of a sequence\n",
    "- Task #2: given a dataset, find A and Pi. This is called \"learning\" or \"training\"\n",
    "- Start considering how to implement these in copmuter code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small modyfications\n",
    "- we will modify how we estimate A and Pi ( previously we used \"maximum likeihood estimation\" )\n",
    "- compute the probability of a sequence \n",
    "\n",
    "Probability of a sequence :\n",
    "\n",
    "- only involves multiplication\n",
    "- what if one of the values is 0? Transition never appeared in trein set\n",
    "- the whole thing becomes 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ADD-ONE SMOOOTHING\n",
    "\n",
    "- give a samall probabilty to every possible transition\n",
    "- add a 'fake count' of 1 for each ( i,j ) transition\n",
    "\n",
    "        {estimate}A>ij = count( i -> j) + 1 / count( i ) + M\n",
    "\n",
    "(adding M to the denominator ensures that each row of A sums to 1)\n",
    "\n",
    "\n",
    "- we can do this for the initial state distribution too\n",
    "\n",
    "        {estimate}Pi>i = count( s>1 = i) + 1 / N + M\n",
    "\n",
    "### add-epsilon smoothing\n",
    "\n",
    "epsilon = 3 odwrocone = e\n",
    "\n",
    "- we can make it more smootvh ( e > 1) or less smooth (e < 1)\n",
    "\n",
    "        {estimate}A>ij = count( i -> j) + e / count( i ) + eM\n",
    "\n",
    "        {estimate}Pi>i = count( s>1 = i) + e / N + eM\n",
    "\n",
    "### computing the Probability of a sequence\n",
    "\n",
    "- this involves multiplying many small numbers together\n",
    "- common to use 20k-50k vocabulary size in english\n",
    "- as you multiply small numbers together, thay approach 0! (0.1 * 0.1 = 0.01)\n",
    "\n",
    "- So what's the solution to this issue? The solution is to work in the log space instead. That is, instead of finding probabilities directly, we can find a log probabilities. Note that it's fine to do this transformation because what we often want to do is compare to probabilities. So for example, if we have two sentences, we want to know which sentence is more likely or if we have to Markov models, we might want to know under which Markov model does the sequence yield the largest probability. Also note that doing this transformation would not change the answer, since the log is a monotonically increasing function.\n",
    "\n",
    "- we dont meed the actual probability value, since waht we usually want to do is compare(e.g is one sequence more likely than another?)\n",
    "- it works because the log is monotonically increasing function\n",
    "- if A > B, then log(A) > log(B)(i.e the \"winner\" is preserved)\n",
    "\n",
    "\n",
    "\n",
    "- Does it work? log>10(10^-10) = -10\n",
    "\n",
    "- Does it work? log>10(10^-100) = -100\n",
    "\n",
    "\n",
    "#### workin with log probabilities\n",
    "\n",
    "- since log(AB) = log(A) + log(B)\n",
    "- if we dont need the probabilities we can store only the logged values\n",
    "- NOTE: do not compute the product and then take the log ( u would end up with taking the log of zero which doesnt solve a problem) solution: take the sum of the log probabilities instead\n",
    "- NOTE: addition (+) is more efficient( faster ) then multiplication\n",
    "so use the log for eficciency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Text Classifier\n",
    "\n",
    "- text classifier is a model taht will take as input a string of text and output a prediction about the category it belongs to\n",
    "\n",
    "\" all data is the same \"\n",
    "\n",
    "\n",
    "    Poem --> Classifier --> Is the poem by Edgar Allan Pue or Robert Frost\n",
    "\n",
    "    Email -> Classifier --> Spam or not spam\n",
    "\n",
    "    Movie Review --> Classifier --> positive or negative\n",
    "\n",
    "- Any text can be used as input\n",
    "\n",
    "- any clas can be used as a target\n",
    "\n",
    "this is pnly for exercise purpose, if our goal is accuracy, then we might simply use pre-trainde deep neural network\n",
    "\n",
    "- **Text calssification is example of supervised learning**\n",
    "- **Markov models are unsupervised**\n",
    "\n",
    "        p(y|x) = p(x|y)p(y) / p(x)\n",
    "\n",
    "##### so how can we make use of markov models in a supervised context?\n",
    "- Bayes rule is an answer\n",
    "\n",
    "- The basic idea is this suppose that I have two sets of poems. Poems by Robert Frost and poems by Edgar Allan Poe. Now suppose that I train at two separate Markov models for each poet, so each of these will have their own distinct A matrices and PI vectors. As we recall, one task we can perform when we have a Markov model is to figure out the probability of a sequence given A and Pi\n",
    "\n",
    "- nieznany teskt przechodzi przez dwa modele tworcow, gdzie wylicza sie prawdopodobientwo\n",
    "\n",
    "new unknown text --> Markov model (A>0, Pi>0) --> p(x | author = Frost)\n",
    "\n",
    "                 --> Markov model (A>1, Pi>1) --> p(x | author = Poe)\n",
    "\n",
    "#### applying Bayes' Rule\n",
    "- we have p( poem | author ), but we want p(author | poem)\n",
    "- why?\n",
    "- then we can apply the following decision rule:\n",
    "\n",
    "- \"poem\" = x \"author\" = class\n",
    "\n",
    "        k* = arg max p(class = k | x)\n",
    "\n",
    "- p(author|poem) = p(poem|author)p(author) / p(poem)\n",
    "\n",
    "### RECAP\n",
    "\n",
    "- we train a separate markov models for each class\n",
    "- each model gives us  p(x | class = k)  for all k\n",
    "- general form of decision rule using Bayes' rule: k* = argmax>k p(class = k |x)\n",
    "- posterior can be simplified since we dont need its actual value\n",
    "- maximum a posteriori(MAP): k*=argmax>k logp(x | class = k) + logp(class=k)\n",
    "- Maximum likeihood:k*=argmax>k logp(x | class = k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "- u will be given poeams by 2 authors: Edgar allan Poe  and Robert Frost\n",
    "- build a classifier that can distinguish between the 2 authors\n",
    "- compute train and test accuracy\n",
    "- check for class imbalance, compute F1 - score if imbalanced\n",
    "\n",
    "#### details\n",
    "1. loop through each file, save each line to a list( one line == one sample)\n",
    "2. save the labels too\n",
    "3. train-test split\n",
    "( at this potn our input data is still lines of text - we need to implement markoc transition matrix where each state represents a word in a vocabulary)\n",
    "we need to know which indicies in the matrix correspond to which words\n",
    "4. create a mapping from unique word to unique integer index( collecting all the unique words). \n",
    "5. loop through data, tokenize each line( string split should suffice)\n",
    "6. assign each unique word a unique integer index ( From these unique words will be able to assign each of them a unique integer index, which will then be used as indicies into markov matrix )\n",
    "**when we use the word mapping typically that refers to a python dictionary**\n",
    "7. create a special index for uknknown words( words in the test set but not the train set )\n",
    "8. convert each line of text(the samples) into integer lists \n",
    "9. train a markov model for each class( allan poe/robert frost)(each model should be trainde on lines of poeam only for their rescpective poet)\n",
    "10. dont forget to use smoothin( e.g add-one smoothing)\n",
    "11. consider whether you need A and Pi, or log(A) and log(Pi)\n",
    "12. for bayes' rule, compute the priors: p(class=k)\n",
    "\n",
    "now we have everything for our prediction - your data is stored as integers, models are trained ane we have the priors for each class\n",
    "\n",
    "13. write a function to compute the posterior(?) for each class, given an input (it should take in an input sequence and it should compute the log posterior for each class)\n",
    "\n",
    "14. take the argmax over the posteriors to get predicted class for that input sequence( remember that there is no need to compute the full posterior since the denominator is constant for each class )\n",
    "\n",
    "15. now make predictions for both train and test sets\n",
    "\n",
    "16. compute ac dor train/test\n",
    "\n",
    "16. as a bonus check whether or not the classes are imbalanced ( this information is ine the priors )\n",
    "\n",
    "17. if imbalanced, check confusion matrix and F-1 score( compute the confusion matrix to see which class has incorrect predictions most often, furthermore, you may want to compute a metric like F1 score, which takes into account any imbalance )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_files = [\n",
    "  'edgar_allan_poe.txt',\n",
    "  'robert_frost.txt',\n",
    "]\n",
    "\n",
    "!head edgar_allan_poe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into lists\n",
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for label, f in enumerate(input_files):\n",
    "  print(f\"{f} corresponds to label {label}\")\n",
    "\n",
    "  for line in open(f):\n",
    "    line = line.rstrip().lower()\n",
    "    if line:\n",
    "      # remove punctuation\n",
    "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "      input_texts.append(line)\n",
    "      labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Ytrain), len(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "word2idx = {'<unk>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate word2idx\n",
    "for text in train_text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "      if token not in word2idx:\n",
    "        word2idx[token] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into integer format\n",
    "train_text_int = []\n",
    "test_text_int = []\n",
    "\n",
    "for text in train_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx[token] for token in tokens]\n",
    "  train_text_int.append(line_as_int)\n",
    "\n",
    "for text in test_text:\n",
    "  tokens = text.split()\n",
    "  line_as_int = [word2idx.get(token, 0) for token in tokens]\n",
    "  test_text_int.append(line_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_int[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize A and pi matrices - for both classes\n",
    "V = len(word2idx)\n",
    "\n",
    "A0 = np.ones((V, V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V, V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute counts for A and pi\n",
    "def compute_counts(text_as_int, A, pi):\n",
    "  for tokens in text_as_int:\n",
    "    last_idx = None\n",
    "    for idx in tokens:\n",
    "      if last_idx is None:\n",
    "        # it's the first word in a sentence\n",
    "        pi[idx] += 1\n",
    "      else:\n",
    "        # the last word exists, so count a transition\n",
    "        A[last_idx, idx] += 1\n",
    "\n",
    "      # update last idx\n",
    "      last_idx = idx\n",
    "\n",
    "\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 0], A0, pi0)\n",
    "compute_counts([t for t, y in zip(train_text_int, Ytrain) if y == 1], A1, pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize A and pi so they are valid probability matrices\n",
    "# convince yourself that this is equivalent to the formulas shown before\n",
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log A and pi since we don't need the actual probs\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute priors\n",
    "count0 = sum(y == 0 for y in Ytrain)\n",
    "count1 = sum(y == 1 for y in Ytrain)\n",
    "total = len(Ytrain)\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)\n",
    "p0, p1\n",
    "\n",
    "# P0 is about 33 percent and P one is about sixty six percent because of this, it wouldn't make sense to use the maximum likelihood method.\n",
    "# Instead, we should look at the posterior, which corresponds to the map method.\n",
    "# Note also that because the classes are slightly imbalanced, we may want to use a metric other than\n",
    "# the accuracy to evaluate a classifier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "class Classifier:\n",
    "  def __init__(self, logAs, logpis, logpriors):\n",
    "    self.logAs = logAs\n",
    "    self.logpis = logpis\n",
    "    self.logpriors = logpriors\n",
    "    self.K = len(logpriors) # number of classes\n",
    "\n",
    "  def _compute_log_likelihood(self, input_, class_):\n",
    "    logA = self.logAs[class_]\n",
    "    logpi = self.logpis[class_]\n",
    "\n",
    "    last_idx = None\n",
    "    logprob = 0\n",
    "    for idx in input_:\n",
    "      if last_idx is None:\n",
    "        # it's the first token\n",
    "        logprob += logpi[idx]\n",
    "      else:\n",
    "        logprob += logA[last_idx, idx]\n",
    "      \n",
    "      # update last_idx\n",
    "      last_idx = idx\n",
    "    \n",
    "    return logprob\n",
    "  \n",
    "  def predict(self, inputs):\n",
    "    predictions = np.zeros(len(inputs))\n",
    "    for i, input_ in enumerate(inputs):\n",
    "      posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] \\\n",
    "             for c in range(self.K)]\n",
    "      pred = np.argmax(posteriors)\n",
    "      predictions[i] = pred\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array must be in order since classes are assumed to index these lists\n",
    "clf = Classifier([logA0, logA1], [logpi0, logpi1], [logp0, logp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrain = clf.predict(train_text_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain == Ytrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptest = clf.predict(test_text_int)\n",
    "print(f\"Test acc: {np.mean(Ptest == Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# read about F-score: https://en.wikipedia.org/wiki/F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Ytrain, Ptrain)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(Ytest, Ptest)\n",
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytrain, Ptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Ytest, Ptest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using markov models to Generate Text\n",
    " - classifying text: supervised learning ( we have targets / labels)\n",
    " - generating text : ussupervised learning ( no labels )\n",
    "\n",
    "BAYES CLASSIFIER:\n",
    "\n",
    "- Discriminative Model = p(y|x) - Examples: logistic regression, neural networks)\n",
    "\n",
    "- generative Model = p(x|y) - they can be used to generate text\n",
    "\n",
    "### Sampling(drawing random numbers)\n",
    "- sampling from N(0,1) : **np.random.randn()**\n",
    "- sampling from Brenoulli : **np.random.choice([0,1]) or np.random.choice(2)**\n",
    "- sampling from categorical(e.g a 10-sided die): **np.random.choice(10)**\n",
    "- Bernoulli with p(heads) = 0.8 (where heads = 1): **np.random.choice([0,1], p = [0.2,0.8])**\n",
    "- sampling words - suppose our vocabulary is (\"cat\", \"dog\", \"mouse\")\n",
    "- with probabilities 0.2, 0.5, 0.3\n",
    "- supposing we can map the words to ints (0, 1, 2), then its easy: np.random.choice(3,p= [0.2,0.5,0.3])\n",
    "\n",
    "\n",
    "#### problems qith the markov assumption\n",
    "- recall: the next word depends only on a single preceding word\n",
    "\n",
    "#### Extending markov model ( second order markov)\n",
    "- instead of depending on only one past state, depend on two \n",
    "        p(s>t | s>t-1, s>t-2, ...) = p(s>t | s>t-1, s>t-2)\n",
    "\n",
    "#### 2nd-Order Markov Model Implementation\n",
    "\n",
    "        A> ijk = p(s>t = k | s>t-1 = j, s>t-2 = i)\n",
    "\n",
    "- this is a 3D array( sometimes reffered to as a **\"tensor\"** in ML)\n",
    "- its shape is MxMxM = M^3\n",
    "- 3rd-order: M^4, 4th order: M^5 etc. This is exponential growth!\n",
    "\n",
    "#### the full model\n",
    "\n",
    "- Pi>i = p(s>1 = i)\n",
    "- A(1)>ij = p(s>2 = j | s>1 = i)\n",
    "- A(2)>ijk = p(s>t = k | s>t-1 = j, s>t-2 = i)\n",
    "\n",
    "\" The(Pi) quick(A1) brown(A2) fox(A2) jumps(A2) over(A2) the(A2) lazy(A2) dog(A2)\"\n",
    "\n",
    "#### Preview / Foreshadowing\n",
    "- in deep learning, there is no need to make the markov assumption\n",
    "- magically, there is \"no\" increase in computational cost when you add more previous words\n",
    "     -  in actuality, it will depend on the type of neural network. For now, assume a linear increase, which is inconsequential(niekonsekwentny, nieistotny) compared to exponential increase\n",
    "\n",
    "\n",
    "\n",
    "#### storing word probabilities in dictionaries:\n",
    " \n",
    "pi = {\n",
    "        \"the\": 0.2,\n",
    "        \"a\": 0.3,\n",
    "        \"is\": 0.1,\n",
    "        ...\n",
    "}\n",
    "\n",
    "- we will not use add-one smoothing, so many words will have zero probability (and they will simply not appear in the dictionary)\n",
    "- thus, although there will be V words in the vocabulary, the dictionary will not store V items\n",
    "\n",
    "#### why in disctionaries?\n",
    "- sparsity\n",
    "- we use add-one smoothing since many possible transitions dont appear in the training corpus\n",
    "- this gets worse as the model order gets larger\n",
    "- i.e Transition array will be mostly zeros (out of V x V x V total elements) (if we use a dictionary that only stores the words that we have actually seen in a sequence, then the amount of values we need to store will be much smaller.)\n",
    "   - its more efficient to store only the transitions which were seen\n",
    "\n",
    "#### first order Transitions\n",
    "- ( used only the model the 2nd word in each line)\n",
    "\n",
    " \n",
    "A1 = {\n",
    "        \"the\": {    <--- word at time t-1\n",
    "\n",
    "                \"cat\": 0.05,   <--- p(cat|the) = 0.05\n",
    "                \"dog\": 0.03,   <--- words at time t\n",
    "                \"mouse\": 0.01,\n",
    "                ...\n",
    "        }\n",
    "\n",
    "        \"a\": { ... }\n",
    "        \n",
    "        ...\n",
    "}\n",
    "\n",
    "#### sampling from a probability dictionary:\n",
    "- how do we sample words from a dictionary with probabilities stores as dictionary values( with words as keys)?\n",
    "\n",
    "example:\n",
    "\n",
    "start by drawing a number x = rnadom U(0,1)\n",
    "\n",
    "probs = {\n",
    "\n",
    "        \"a\": 0.2,\n",
    "        \"b\": 0.5,\n",
    "        \"c\": 0.3\n",
    "}\n",
    "\n",
    "- if 0 < x < 0.2, then \"a\"\n",
    "- if 0.2 < x < 0.7, then \"b\"\n",
    "- if 0.7 < x < 1, then \"c\"\n",
    "\n",
    "- the area covered by each of these ranges is exactly the assigned probabilities(0.2,0.5 and 0.3)\n",
    "- we must  calculate the cumulative sum\n",
    "    - 0 + 0.2: first boundary\n",
    "    - 0.2 + 0.5: = 0.7: second boundary\n",
    "    - 0.7 + 0.3 = 1: final boundary\n",
    "\n",
    "#### alternative\n",
    "- conceptually simpler but less ideal: just convert the dictonary keys and values into lists, then use np.random.choice()\n",
    "- but it doesnt give you prctisce with a critical skill for data science\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} # start of a phrase\n",
    "first_order = {} # second word only\n",
    "second_order = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "  if k not in d:\n",
    "    d[k] = []\n",
    "  d[k].append(v)\n",
    "\n",
    "# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what the function does?\n",
    "\n",
    "Phrases in dataset:\n",
    "- im happy\n",
    "- im sad\n",
    "- im content \n",
    "- im happy\n",
    "- im happy\n",
    "- im hungry\n",
    "\n",
    "Desired result:\n",
    "\n",
    "    {()\"I\", \"am\"): [\"happy\", \"sad\", \"content\", \"happy\", \"happy\", \"hungry\"]}\n",
    "\n",
    "Later:\n",
    "\n",
    "    {()\"I\", \"am\"): {\"happy\": 0.5, ...}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('robert_frost.txt'):\n",
    "  tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "\n",
    "  T = len(tokens)\n",
    "  for i in range(T):\n",
    "    t = tokens[i]\n",
    "    if i == 0:\n",
    "      # measure the distribution of the first word\n",
    "      initial[t] = initial.get(t, 0.) + 1 \n",
    "    else:\n",
    "      t_1 = tokens[i-1]\n",
    "      if i == T - 1:\n",
    "        # measure probability of ending the line\n",
    "        add2dict(second_order, (t_1, t), 'END')\n",
    "      if i == 1:\n",
    "        # measure distribution of second word\n",
    "        # given only first word\n",
    "        add2dict(first_order, t_1, t)\n",
    "      else:\n",
    "        t_2 = tokens[i-2]\n",
    "        add2dict(second_order, (t_2, t_1), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...]\n",
    "# into {cat: 0.5, dog: 0.4, mouse: 0.1}\n",
    "\n",
    "def list2pdict(ts):\n",
    "  # turn each list of possibilities into a dictionary of probabilities\n",
    "  d = {}\n",
    "  n = len(ts)\n",
    "  for t in ts:\n",
    "    d[t] = d.get(t, 0.) + 1\n",
    "  for t, c in d.items():\n",
    "    d[t] = c / n\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_1, ts in first_order.items():\n",
    "  # replace list with dictionary of probabilities\n",
    "  first_order[t_1] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, ts in second_order.items():\n",
    "  second_order[k] = list2pdict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "  # print \"d:\", d\n",
    "  p0 = np.random.random()\n",
    "  # print \"p0:\", p0\n",
    "  cumulative = 0\n",
    "  for t, p in d.items():\n",
    "    cumulative += p\n",
    "    if p0 < cumulative:\n",
    "      return t\n",
    "  assert(False) # should never get here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "#\n",
    "# Determine the vocabulary size (V)\n",
    "# We know that pi has shape V, A1 has shape V x V, and A2 has shape V x V x V\n",
    "#\n",
    "# In comparison, how many values are stored in our dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2:\n",
    "# We can skip the step where we accumulate all the possible next words in a list\n",
    "# E.g. [cat, cat, dog, dog, dog, ...]\n",
    "#\n",
    "# Instead, like we do with the initial state distribution, create the dictionary\n",
    "# of counts directly as you loop through the data.\n",
    "#\n",
    "# You'll no longer need list2pdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "  for i in range(4): # generate 4 lines\n",
    "    sentence = []\n",
    "\n",
    "    # initial word\n",
    "    w0 = sample_word(initial)\n",
    "    sentence.append(w0)\n",
    "\n",
    "    # sample second word\n",
    "    w1 = sample_word(first_order[w0])\n",
    "    sentence.append(w1)\n",
    "\n",
    "    # second-order transitions until END\n",
    "    while True:\n",
    "      w2 = sample_word(second_order[(w0, w1)])\n",
    "      if w2 == 'END':\n",
    "        break\n",
    "      sentence.append(w2)\n",
    "      w0 = w1\n",
    "      w1 = w2\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY\n",
    "- basic idea: predict the future from the past\n",
    "- predict x(t from x(t-1, x(t-2),..., x(1)\n",
    "- the markov model cuts this off at (x(t-1)\n",
    "- we can add more term: 1st-order Markov, 2nd-order Markov\n",
    "- time series analysis: forecast the next value from previous values\n",
    "   - can use linear regression, neural networks, random forest etc\n",
    "- \"autoregressive text models\" with more complex architectures\n",
    "- E.g RNNs (sush as LSTMs) use the same training strategy\n",
    "- the most advanced NLP models(transformers) use the same strategy!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrypting Ciphers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cipher Decryption\n",
    "- unique project - not normaly seen in NLP\n",
    "- Applies multiple important topics:\n",
    "   - probabilistic language modeling\n",
    "   - genetic algorithms \n",
    "- they are immediately applicable in the real world\n",
    "   - for example: warfare, espionage\n",
    "\n",
    "### Section Outline\n",
    "\n",
    "* What is a cipher?\n",
    "    - encode and decode a message\n",
    "* Language Modeling\n",
    "    - \"what is the probability of this sentence?\"\n",
    "    - later: can generate new sentences( poetry, article spinning )\n",
    "* Genetic algorithm / evolutionary algorithm\n",
    "    - Optimization based on biological evolution\n",
    "\n",
    "* We have to:\n",
    "    - understand a basic cipher and how to implement that in code\n",
    "    - understand a probabilistic language models\n",
    "    - we have to understamd genetic algorithms, which is what will allow our prediction of the encryption code to envolve and match our model of the english language, in other words \n",
    "* our decoded message should have the highest likeihood if the model is trained on the english language \n",
    "    - a message not in english should have smaller likeihood ( \"maximum likeihood\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spam detetor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing\n",
    "\n",
    "columnas 1-48\n",
    "\n",
    "- word-frequency measure - number of times word appears divided by number of words in document x 100\n",
    "\n",
    "last column is a label\n",
    "\n",
    "- 1 = spam, 0 = not smap\n",
    "\n",
    "one example of a \" term-document matrix \" - terms go along columns, documents (aka emails) go along rows\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes spam detection for NLP class, which can be found at:\n",
    "# https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python\n",
    "# https://www.udemy.com/data-science-natural-language-processing-in-python\n",
    "# dataset: https://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "# Author: http://lazyprogrammer.me\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Note: technically multinomial NB is for \"counts\", but the documentation says\n",
    "#       it will work for other types of \"counts\", like tf-idf, so it should\n",
    "#       also work for our \"word proportions\"\n",
    "\n",
    "data = pd.read_csv('spambase.data').values # use pandas for convenience\n",
    "np.random.shuffle(data) # shuffle each row in-place, but preserve the row\n",
    "\n",
    "X = data[:,:48]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for NB:\", model.score(Xtest, Ytest))\n",
    "\n",
    "\n",
    "\n",
    "##### you can use ANY model! #####\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for AdaBoost:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- all machine learning interfaces are rhe same - one line change\n",
    "- Whats important: identify that its a classification task, choose a classification model\n",
    "- naive bayes is weak, adaBoost is strong (although incidentally NB performs exceptionally well on text)\n",
    "\n",
    "\n",
    "### central message\n",
    "- NLP is the application of Ml of text, not ML itself\n",
    "- if we were learning the ML algorithms, you would be having much tougher time\n",
    "- consider the current layer of abstraction\n",
    "- at this level you dont want to think about:\n",
    "    - how do I implement a decision tree?\n",
    "- you shoul think:\n",
    "    - how to use the decision tree?:\n",
    "\n",
    "### Layers of abstractiom:\n",
    "\n",
    "Hardware --> Numerical Computing --> ML(uses Numpy, Scipy, ...) --> NLP(uses ML)\n",
    "\n",
    "- At the \"ML level\", you think about:\n",
    "    - Theory behind ML algorithms\n",
    "    - How to implement them in code\n",
    "- use libraries like numpy, scipy\n",
    "- but you would not think about:\n",
    "    \"how does Numpy work?\"\n",
    "\n",
    " * at the Numerical computing layer:\n",
    " * Numpy and Scipy specialize in doing extremely fast linear algebra:\n",
    "    - matrix multiplication\n",
    "    - solving linear equations\n",
    "    - storing vectors, matrices, multidimensional arrays efficiently\n",
    "* most data scientist dont have to think about this \n",
    "* E.g you dont want to think about:\n",
    "    - \"How do i perform a dot product efficiently?\"\n",
    "When Implementing decision tree\n",
    "\n",
    "- The lowet layer is hardware - how do we physically build a machine that does fast calculations?\n",
    "- Ex GPU does matrix operations much faster than CPU\n",
    "- Google invented TPU for deep learning to go even faster\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "- 2 parts: \"Naive\", \"Bayes\"\n",
    "- We will discuss \"Bayes\" first, then what \"Naive:\" means\n",
    "- Bayes rule:\n",
    "\n",
    "- p(y|x) = p(x,y) / p(x)\n",
    "\n",
    "x = input feature ( e.g [% of time \"insurance\" appeared, % of time \"pill\" appeared, ...])\n",
    "\n",
    "y = target (e.g \"spam\" / \"not spam\")\n",
    "\n",
    "- in general we ca use the \"argmax\"\n",
    "- we can also drop p(x) since it doeasnt depend on y\n",
    "\n",
    "        k* = argmax>k { p(y=k|x)}\n",
    "        k* = argmax>k { p(x|y = k)p(y = k) / p(x) }\n",
    "        k* = argmax>k { p(x|y = k)p(y = k) }\n",
    "\n",
    "- How do we calculate these probabilities?\n",
    "    - we need to calculate 2 things: likeihood and prior\n",
    "\n",
    "    k* = argmax>k  {p(x|y = k)p(y = k)}\n",
    "\n",
    "         p(y = spam) = # spam emails / # total emails\n",
    "         p(y = not spam) = # not spam emails / # total emails\n",
    "\n",
    "### Likeihood\n",
    "- we can choose a likeihood distribution thats approppriate for our feature vectors\n",
    "- E.g if real-valued / bell-curve --> Gaussian ( they re real valued and they follow a bell curve shape)\n",
    "- if counts --> Multinominal\n",
    "- If binary --> Bernoulli  (Zero or one)\n",
    "- in SKLearn: GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "### What does \"Naive\" mean?\n",
    "\n",
    "- recall we have feature vector of size D: [x>1,x>2,x>3,...,x>D]\n",
    "- Naive = we assume each component is independent given Y\n",
    "- if 2 RVs are independent: p(A,B) = p(A)p(B)\n",
    "\n",
    "#### the naive assumption\n",
    "- is it appropriate? - no\n",
    "- if an email contains the word \"insurance\", its more likely to contain other financial-related terms e.g \"loan\",\"money\", etc\n",
    "- if an email contains the word \"Viagra\", it is probably less likely to contain words like \"insurance\" or \"loan\"\n",
    "- yet, Naive Bayes performs suprisingly well with word frequencies\n",
    "\n",
    "example\n",
    "- 2 classes: spam/not spam\n",
    "    - p(y - spam) = 0.4\n",
    "    - p(y = not spam) = 0.6\n",
    "\n",
    "- Feature vector will have 2 components, Bernoulli(word appears or it doeasnt)\n",
    "    - Feature #1: insurance\n",
    "    - Feature #2: Drugs\n",
    "\n",
    "\n",
    "How many parameters do we need?\n",
    "\n",
    "2 features x 2 classes x 1 parameter per Bernoulli distribution:\n",
    "\n",
    "    p(insurance = 1 | y = spam) = 0.9\n",
    "    p(drugs = 1 | y = spam) = 0.8\n",
    "\n",
    "    p(insurance = 1 | y = not spam) = 0.2\n",
    "    p(drugs = 1 | y = not spam) = 0,1\n",
    "\n",
    "#### make a prediction\n",
    "- suppose an email contains the word \"insurance\" but not \"drugs\"\n",
    "- do we classify it as a spam or not?\n",
    "- for spam class:\n",
    "    - p(insurance = 1 | y = spam)*p(drugs = 0 | y = spam)*p(spam) = 0.9*0.2*0.4 = 0.072\n",
    "- for not spam class:\n",
    "    - p(insurance = 1 | y = not spam)*p(drugs = 0 | y = not spam)*p(not spam) = 0.2*0.9*0.6 = 0.1O8\n",
    "- we choose \"not spam\"\n",
    "\n",
    "        k* = argmax>k {p(x | y = k)p(y = k)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdBOOST\n",
    "\n",
    "- combine many weak classifiers to form one strong classifier (zobacz se w googlu na rysunki)\n",
    "- in general, the approach of combining models is called \"ensembling\"\n",
    "- other ways to ensemble: bagging, random forest\n",
    "- adaBoost is unique - it wants its base classifiers to be weak\n",
    "\n",
    "### what is weak classifier?\n",
    "- linear classifier is an example\n",
    "    - cant solve XOR or donut ( kwadratowa szachownica / kropki zolte wokol niebisekisch )\n",
    "- decision stomps are even weaker! ( we will explain the name later)\n",
    "    - so weak these lines cant even be rotated at an angle \n",
    "\n",
    "#### decision stup in code \n",
    "- esiest shit\n",
    "\n",
    "if insurance_count > 3:\n",
    "    return \"spam\"\n",
    "else:\n",
    "    return \"not spam\"\n",
    "\n",
    "#### why decision \"stump\"?\n",
    "- is the simplest form of decision tree\n",
    "- a stump is a tree with just 1 question\n",
    "\n",
    "h > 1.8km --> no --> female\n",
    "          --> yes --> next question\n",
    "\n",
    "#### decision Boundary \n",
    "- for a decision tree, since we can only split acros 1 variable at a time, the decision boundary is always aligned with the axes \n",
    "\n",
    "## Adaboost\n",
    "- the trick is that we weight each individual weak classifier\n",
    "- the secret sauce of AdaBoost is how thsese weights are found\n",
    "\n",
    "\n",
    "### No-math explanation:\n",
    "- AdaBoost builds up each individual decision stump one at a time\n",
    "- At the \"M-1\"th stage, the classifier will consist of M-1 individual decision stumps\n",
    "- Our job at that time, is to find the next decision stump, and its corresponding weight\n",
    "- The key is to also weight each training sample\n",
    "- the samples that the (full) model currently gets the \"most wrong\" are given the highest weight\n",
    "- the newest decision stump prioritizes getting those samples right\n",
    "- in other words we are always improving on what we are currently not good at \n",
    "\n",
    "### other types of features \n",
    "(most of these fit into what we call \"bag of words\")\n",
    "\n",
    "- word proportion (we already saw this)\n",
    "- raw word counts\n",
    "- binary( 1 if word appears, 0 otherwise)\n",
    "- TF-IDF (takes into account the fact that some words appear in many documents, and hence dont really tell us much e.g and, in , a)\n",
    "\n",
    "### Trade-offs\n",
    "- example:\n",
    "    - Model 1 has 70% accuracy an takes 100ms to train\n",
    "    - Model 1 has 71% accuracy an takes 100 hours to train\n",
    "    - maybe 1% gain in accuracy earns you $5, but costs you $1000 to train on AWS\n",
    "\n",
    "### Dont Overfit\n",
    "- really complex model with many parameters may not be good\n",
    "- \"bias-variance\" tradeoff\n",
    "- etreme example: 100% train accuracy, 51% test accuracy\n",
    "- simple models tend to overfit less"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of data\n",
    "- supervised learning:\n",
    "    - inputs: X(shape = N x D)\n",
    "    - targets: Y(Shape = N)\n",
    "- Unsupervised learning:\n",
    "    - X(shape = N x D)\n",
    "\n",
    "NOTE: for multi-class classification T would be NxK with one-hot encoded targets. SciKit-learn does the encoding for you, so you only need to pass in the actua labels\n",
    "\n",
    "- feature is just how many times that word appeared in that document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS spam collection dataset\n",
    "- (pretty much ecery script in every course is going to follow this outline)\n",
    "\n",
    "    - #1 Load in the data\n",
    "    - #2 apply a machine learning algorithm on the data( if you need suggestions, use the classifiers from the previous example)\n",
    "    - #3 Look at the accuracy ( is this a good classifier?)\n",
    "\n",
    "- Preprocessing ( between #1 and #2)\n",
    "    - see:\n",
    "        - sklearn.feature_extraction.text.TfidfVectorizer (into TF-IDF format )\n",
    "        - sklearn.feature_extraction.text.CountVectorizer (raw counts format )\n",
    "    - same API as unsupervised models:\n",
    "        \n",
    "        obj = TfidfVectorizer()\n",
    "        obj.fit(sentences)\n",
    "        X = obj.transform(sentences)\n",
    "        X = obj.fit_transform(sentences) #all in one step\n",
    "\n",
    "### Pay attentio to these things\n",
    "- Is TF-IDF better than raw counts or vice versa?\n",
    "- Look at the different options for TF-IDF\n",
    "- which classifier is best for those features? ( dont just assume a more complex model will be better)\n",
    "\n",
    "\n",
    "- later we will be writing our own feature extraction and modeling code \n",
    "- idea is to strip away the training wheels slowly\n",
    "- you want to be comfortable both with custom codig and using libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python\n",
    "# https://www.udemy.com/data-science-natural-language-processing-in-python\n",
    "\n",
    "# Author: http://lazyprogrammer.me\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# data from:\n",
    "# https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "# file contains some invalid chars\n",
    "# depending on which version of pandas you have\n",
    "# an error may be thrown\n",
    "df = pd.read_csv('../large_files/spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to something better\n",
    "df.columns = ['labels', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary labels\n",
    "df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})\n",
    "Y = df['b_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(df['data'], Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try multiple ways of calculating features\n",
    "tfidf = TfidfVectorizer(decode_error='ignore')\n",
    "Xtrain = tfidf.fit_transform(df_train)\n",
    "Xtest = tfidf.transform(df_test)\n",
    "\n",
    "# count_vectorizer = CountVectorizer(decode_error='ignore')\n",
    "# Xtrain = count_vectorizer.fit_transform(df_train)\n",
    "# Xtest = count_vectorizer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we get back a sparse matrix instead of the usual Nampara, where you can see all the numbers, as you recall, this is because the array contains a lot of zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model, train it, print scores\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
    "print(\"test score:\", model.score(Xtest, Ytest))\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "def visualize(label):\n",
    "  words = ''\n",
    "  for msg in df[df['labels'] == label]['data']:\n",
    "    msg = msg.lower()\n",
    "    words += msg + ' '\n",
    "  wordcloud = WordCloud(width=600, height=400).generate(words)\n",
    "  plt.imshow(wordcloud)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('spam')\n",
    "visualize('ham')\n",
    "\n",
    "#or each of the classes, we'll be able to see what are the most common words in a spam/ham message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what we're getting wrong\n",
    "X = tfidf.transform(df['data'])\n",
    "df['predictions'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things that should be spam\n",
    "sneaky_spam = df[(df['predictions'] == 0) & (df['b_labels'] == 1)]['data']\n",
    "for msg in sneaky_spam:\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things that should not be spam\n",
    "not_actually_spam = df[(df['predictions'] == 1) & (df['b_labels'] == 0)]['data']\n",
    "for msg in not_actually_spam:\n",
    "  print(msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what is it\"\n",
    "    - sentiment = how positive or negative some text is\n",
    "    - amazon reviews, yelp reviews, hotel reviews, tweets\n",
    "\n",
    "Note: our data is in XML format\n",
    "\n",
    "## outline:\n",
    "- we will just look at the electronics category, but we can try the same code on others\n",
    "- we could use 5 star targets to do regression, but lets just do classification since they already marked \"positive\" and \"negative\"\n",
    "- XML parser (BeautifulSoup)\n",
    "- only look at key \"review_text\"\n",
    "- we will need 2 passes, one to determine vocabulary size and which index corresponds to which word, and one to create data vectors\n",
    "- after that, we can just use any SKLearn classifier as we did previously\n",
    "- but wi will use logistic regression so we can interpret the weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERMS\n",
    "\n",
    " **discriminative/condtional models** - Modele dyskryminacyjne, zwane również modelami warunkowymi, to klasa modeli logistycznych używanych do klasyfikacji lub regresji. Rozróżniają granice decyzji na podstawie zaobserwowanych danych, takich jak pass/fail, wygrana/przegrana, żywy/martwy lub zdrowy/chory.\n",
    "\n",
    " In the case of discriminative models, to find the probability, they directly assume some functional  form for P(Y|X) andthen estimate the parameters of P(Y|X) with the help of the training data.\n",
    "\n",
    "they learn the boundaries between classes or labels in a dataset.\n",
    "\n",
    "Discriminative models focus on modeling the decision boundary between classes in a classification problem. The goal is to learn a function that maps inputs to binary outputs, indicating the class label of the input. Maximum likelihood estimation is often used to estimate the parameters of the discriminative model, such as the coefficients of a logistic regression model or the weights of a neural network.\n",
    "\n",
    "Discriminative models (just as in the literal meaning) separate classes instead of modeling the conditional probability and don’t make any assumptions about the data points. But these models are not capable of generating new data points. Therefore, the ultimate objective of discriminative models is to separate one class from another.\n",
    "\n",
    "If we have some outliers present in the dataset, discriminative models work better compared to generative models i.e., discriminative models are more robust to outliers. However, one major drawback of these models is the misclassification problem, i.e., wrongly classifying a data point.\n",
    "\n",
    " \n",
    "\n",
    "Discriminative models divide the data space into classes by learning the boundaries, whereas generative models understand how the data is embedded into the space. Both the approaches are widely different, which makes them suited for specific tasks.\n",
    "\n",
    "**generative models** - Generative models are considered a class of statistical models that can generate new data instances. These models are used in unsupervised machine learning as a means to perform tasks such as\n",
    "\n",
    "- Probability and Likelihood estimation,\n",
    "- Modeling data points\n",
    "- To describe the phenomenon in data,\n",
    "- To distinguish between classes based on these probabilities.\n",
    "\n",
    "Since these models often rely on the Bayes theorem to find the joint probability, generative models can tackle a more complex task than analogous discriminative models.\n",
    "\n",
    "In simple words, a discriminative model makes predictions on unseen data based on conditional probability and can be used either for classification or regression problem statements. On the contrary, a generative model focuses on the distribution of a dataset to return a probability for a given example.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/\n",
    "\n",
    "**poserior probability** - Prawdopodobieństwo a posteriori – prawdopodobieństwo zdarzenia określane po wzbogaceniu wiedzy o nim na bazie obserwacji empirycznych. Prawdopodobieństwo a posteriori wyliczane jest na bazie wzoru Bayesa: prawdopodobieńst a priori oraz prawdopodobieństw warunkowych.\n",
    "\n",
    "\n",
    "{3}P(A|B) = {1}}P(B|A).{2}P(A) / {4}P(B)\n",
    "\n",
    "- **LIKELIHOOD** - 1.the probability of \"B\" being True, given \"A\" is True\n",
    "- **PRIOR** - 2.The probability \"A\" being True. This is knowledge\n",
    "- **POSTERIOR** - 3.The probability of \"A\" being True, given \"B\" is True\n",
    "- **MARGINALIZATION/EVIDENCE** - 4.The probability \"B\" being True\n",
    "\n",
    "\n",
    "**joint probability** - wspolny rozklad prawdopodobientwa -  rodzaj rozkładu prawdopodobieństwa, który zmiennym losowym X, Y, .... przypisuje prawdopodobieństwo, iż zmienne X, Y, .... przyjmą określone wartości (ciągłe lub dyskretne, należące do specyficznych dla każdej z nich zbiorów wartości).\n",
    "\n",
    "Jeśli mamy tylko dwie zmienne losowe, to rozkład nazywamy dwuwymiarowym, a dla większej liczby zmiennych losowych rozkład nazywamy wielowymiarowym.\n",
    "\n",
    "Pomyślmy, że toczymy kostką do gry i uznajmy, że A = 1, jeśli liczba jest parzysta (in. jeśli wytoczymy liczbę 2, 4 lub 6) oraz A = 0, jeśli liczba jest nieparzysta (in. jeśli wytoczymy liczbę 1, 3 lub 5). Ponadto rozważmy, że B = 1, jeśli wytoczymy liczbę pierwszą (czyli 2, 3, 5) oraz B = 0, jeśli wytoczymy inną liczbę.\n",
    "\n",
    "\t1\t2\t3\t4\t5\t6\n",
    "A\t0\t1\t0\t1\t0\t1\n",
    "B\t0\t1\t1\t0\t1\t0\n",
    "\n",
    "Wtedy wspólny rozkład prawdopodobieństwa dla A i B, wyrażony jako funkcja masy prawdopodobieństwa, wynosi\n",
    "\n",
    "P(A=0,B=0) = P{1} = 1/6, P(A=1,B=0) = P{4,6} = 2/6, P(A=0,B=1) = P{3,5} = 2/6, P(A=1,B=1) = P{2} = 1/6\n",
    "\n",
    "What Is a Joint Probability? Joint probability is a statistical measure that calculates the likelihood of two events occurring together and at the same point in time. Joint probability is the probability of event Y occurring at the same time that event X occurs.\n",
    "\n",
    "\n",
    "**Bayes rule** - twierdzenie bayesa - twierdzenie teorii prawdopodobieństwa, wiążące prawdopodobieństwa warunkowe dwóch zdarzeń warunkujących się nawzajem, sformułowane przez Thomasa Bayesa. Twierdzenie stanowi podstawę teoretyczną wnioskowania bayesowskiego, oraz sieci bayesowskich stosowanych w eksploracji danych.\n",
    "\n",
    "Conditional probability is the likelihood of an outcome occurring, based on a previous outcome having occurred in similar circumstances. Bayes' theorem provides a way to revise existing predictions or theories (update probabilities) given new or additional evidence.\n",
    "\n",
    "Applications of Bayes' Theorem are widespread and not limited to the financial realm. For example, Bayes' theorem can be used to determine the accuracy of medical test results by taking into consideration how likely any given person is to have a disease and the general accuracy of the test. Bayes' theorem relies on incorporating prior probability distributions in order to generate posterior probabilities.\n",
    "\n",
    "Prior probability, in Bayesian statistical inference, is the probability of an event occurring before new data is collected. In other words, it represents the best rational assessment of the probability of a particular outcome based on current knowledge before an experiment is performed.\n",
    "\n",
    "Posterior probability is the revised probability of an event occurring after taking into consideration the new information. Posterior probability is calculated by updating the prior probability using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "formula: P(A|B) = P(A ∩ B) / P(B) (iloczyn zbiorow) = P(A) * P(B|A) / P(B)\n",
    "where: P(A) = the probability of A occuring\n",
    "       P(B) = the probability of B occuring\n",
    "\t   P(A|B) = the probability of A given B\n",
    "\t   P(B|A) = the probability of B given A\n",
    "\t   P(A ∩ B) = the probability of both A and B occuring\n",
    "\n",
    "As a numerical example, imagine there is a drug test that is 98% accurate, meaning that 98% of the time, it shows a true positive result for someone using the drug, and 98% of the time, it shows a true negative result for nonusers of the drug.\n",
    "\n",
    "Next, assume 0.5% of people use the drug. If a person selected at random tests positive for the drug, the following calculation can be made to determine the probability the person is actually a user of the drug.\n",
    "\n",
    "(0.98 x 0.005) / [(0.98 x 0.005) + ((1 - 0.98) x (1 - 0.005))] = 0.0049 / (0.0049 + 0.0199) = 19.76%\n",
    "Bayes' Theorem shows that even if a person tested positive in this scenario, there is a roughly 80% chance the person does not take the drug.\n",
    "\n",
    "**sampling** - próbkowanie prawdopodobienstwa / proba losowa\n",
    "\n",
    "\n",
    "\n",
    "In simple random sampling (SRS), each sampling unit of a population has an equal chance of being included in the sample. Consequently, each possible sample also has an equal chance of being selected. To select a simple random sample, you need to list all of the units in the survey population.\n",
    "\n",
    "Probability sampling methods include:\n",
    "- simple random sampling, \n",
    "- systematic sampling, \n",
    "- stratified sampling, \n",
    "- cluster sampling.\n",
    "\n",
    "**Bernoulli sampling**\n",
    "\n",
    "https://www.youtube.com/watch?v=jqBc32NGKn8 \n",
    "\n",
    "\n",
    "**transition state** - The state transition probability matrix of a Markov chain gives the probabilities of transitioning from one state to another in a single time unit. It will be useful to extend this concept to longer time intervals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
